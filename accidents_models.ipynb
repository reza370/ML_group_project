{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import h2o\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from h2o.automl import H2OAutoML\n",
    "from tpot import TPOTClassifier\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_sub = pd.read_csv(\"C:\\\\Users\\\\reza3\\\\OneDrive\\\\Desktop\\\\AIT\\\\Machine learning\\\\group project\\\\data\\\\sub_accidents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4016917, 38)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_sub.drop(columns=\"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Severity', 'Delay(min)', 'Start_Lat', 'Start_Lng', 'Street',\n",
       "       'City', 'County', 'State', 'Zipcode', 'Humidity(%)', 'Crossing',\n",
       "       'Junction', 'Stop', 'Traffic_Signal', 'Sunrise_Sunset',\n",
       "       'Civil_Twilight', 'Start_Year', 'Start_Month', 'Start_Day',\n",
       "       'Start_Hour', 'Start_time', 'IsWeekend', 'Temperature(C)',\n",
       "       'Pressure(cm)', 'Precipitation(cm)', 'Distance(km)', 'Visibility(km)',\n",
       "       'Wind_Speed(kmph)', 'Delay_ln', 'Severity_new', 'Weather_Bin_Clear',\n",
       "       'Weather_Bin_Cloudy', 'Weather_Bin_Rainy', 'Weather_Bin_Snowy',\n",
       "       'Start_Month_December', 'Start_Month_January'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = accidents_sub.drop(columns=['ID', 'Severity', 'Delay(min)', 'Street', 'City', 'County', 'State', 'Zipcode',\n",
    "       'Start_Year', 'Start_Month', 'Start_Day',\n",
    "       'Start_time','Distance(km)', 'Delay_ln', 'Severity_new')\n",
    "Y = accidents_sub.loc[:, ['Severity', 'Severity_new', 'Delay(min)', 'Delay_ln', 'Distance(km)']]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.05)\n",
    "\n",
    "assert len(X_train)  == len(Y_train)\n",
    "assert len(X_test)   == len(Y_test)\n",
    "X.shape, Y.shape,X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"n_estimators\": [10], \n",
    "              \"criterion\": [\"gini\"],\n",
    "              \"max_depth\": 10}\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, Y_train[\"Severity\"])\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test[\"Severity\"], yhat))'''\n",
    "\n",
    "'''grid = GridSearchCV(model, param_grid, refit=True)\n",
    "grid.fit(X_train, Y_train[\"Severity\"])\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "yhat = grid.predict(X_test[\"Severity\"])\n",
    "\n",
    "print(classification_report(Y_test[\"Severity\"], yhat))'''\n",
    "'''from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import  classification_report\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, Y_train[\"Severity\"])\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(\"Ada score: \", accuracy_score(Y_test[\"Severity\"], y_pred))\n",
    "print(classification_report(Y_test[\"Severity\"], y_pred))'''\n",
    "'''import xgboost\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "xgb_reg = xgboost.XGBRegressor(early_stopping_rounds=4) \n",
    "\n",
    "#not improved after 2 iterations\n",
    "xgb_reg.fit(X_train, Y_train[\"Distance(km)\"],\n",
    "                eval_set=[(X_test, Y_test[\"Distance(km)\"])])\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "mse = mean_squared_error(Y_test[\"Distance(km)\"], y_pred)\n",
    "r2 = r2_score(Y_test[\"Distance(km)\"], y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)  #notice we are using mse while xgb uses root mse\n",
    "print(\"R2:\", r2)'''\n",
    "'''import torch\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "inputs = torch.from_numpy(X_train.values)\n",
    "target = torch.from_numpy(Y_train[\"Delay(min)\"].values)\n",
    "print(inputs.size())\n",
    "print(target.size())\n",
    "from torch.utils.data import TensorDataset\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, target)\n",
    "from torch.utils.data import DataLoader\n",
    "# Define data loader\n",
    "batch_size = 500\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "# Build the ANN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(128, input_shape=(X_train.shape[1],), activation='LeakyReLU'))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(64, activation='LeakyReLU'))\n",
    "model.add(Dense(48, activation='LeakyReLU'))\n",
    "model.add(Dense(24, activation='LeakyReLU'))\n",
    "model.add(Dense(12, activation='LeakyReLU'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train[\"Distance(km)\"], epochs=1, batch_size=100, validation_data=(X_test, Y_test[\"Distance(km)\"]))\n",
    "yhat = model.predict(X_test)\n",
    "r2 = r2_score(Y_test[\"Distance(km)\"], yhat)\n",
    "weights_and_biases = model.get_weights()\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, Y_test[\"Distance(km)\"])\n",
    "print(f'Mean Squared Error on Test Set: {loss:.2f}')\n",
    "print(r2)\n",
    "'''\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#sum of squared distances\n",
    "ssd = []\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto')\n",
    "    kmeans.fit(X)\n",
    "    ssd.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, 20), ssd)\n",
    "plt.xticks(range(2, 20))\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"within cluster variation\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.title('Elbow curve')\n",
    "\n",
    "plt.annotate('elbow', xy=(4.3, 220), xytext=(5, 600),  #xytext ---> xy\n",
    "            arrowprops=dict(arrowstyle=\"->\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
